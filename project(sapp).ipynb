{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# Sentiment Analysis Using Python Project\n",
        "\n",
        "## Sentiment Analysis – One of the most popular projects in the industry. Every customer facing industry (retail, telecom, finance, etc.) is interested in identifying their customers’ sentiment, whether they think positive or negative about them.\n",
        "\n",
        "Python sentiment analysis is a methodology for analyzing a piece of text to discover the sentiment hidden within it. It accomplishes this by combining machine learning and natural language processing (NLP). Sentiment analysis allows you to examine the feelings expressed in a piece of text."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## About Sentiment Analysis\n",
        "In this machine learning project, we build a binary text classifier to classify the sentiment behind the text. We use the various NLP preprocessing techniques to clean the data and utilize the LSTM layers to build the text classifier."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Python Sentiment Analysis Dataset\n",
        "The dataset contains more than 14000 tweets data samples classified into 3 types: positive, negative, neutral"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Tools and Libraries used\n",
        "Python – 3.x\n",
        "Pandas – 1.2.4\n",
        "Matplotlib – 3.3.4\n",
        "TensorFlow – 2.4.1\n",
        "To install the above modules into your local machine, run the following command in your command line."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Copy Sentiment Analysis Python Code\n",
        "Please Copy the source code of python sentiment analysis project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "Sentiment Analysis Python Code:\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# ## Sentiment Analysis on US Airline Reviews\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "df = pd.read_csv(\"./Tweets.csv\")ra\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "# In[23]:\n",
        "\n",
        "\n",
        "df.columns\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "tweet_df = df[['text','airline_sentiment']]\n",
        "print(tweet_df.shape)\n",
        "tweet_df.head(5)\n",
        "\n",
        "\n",
        "# In[22]:\n",
        "\n",
        "\n",
        "tweet_df = tweet_df[tweet_df['airline_sentiment'] != 'neutral']\n",
        "print(tweet_df.shape)\n",
        "tweet_df.head(5)\n",
        "\n",
        "\n",
        "# In[21]:\n",
        "\n",
        "\n",
        "tweet_df[\"airline_sentiment\"].value_counts()\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "sentiment_label = tweet_df.airline_sentiment.factorize()\n",
        "sentiment_label\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "tweet = tweet_df.text.values\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(tweet)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "encoded_docs = tokenizer.texts_to_sequences(tweet)\n",
        "padded_sequence = pad_sequences(encoded_docs, maxlen=200)\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "print(tweet[0])\n",
        "print(encoded_docs[0])\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "print(padded_sequence[0])\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "embedding_vector_length = 32\n",
        "model = Sequential() \n",
        "model.add(Embedding(vocab_size, embedding_vector_length, input_length=200) )\n",
        "model.add(SpatialDropout1D(0.25))\n",
        "model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])  \n",
        "print(model.summary()) \n",
        "\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=5, batch_size=32)\n",
        "\n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"Accuracy plot.jpg\")\n",
        "\n",
        "\n",
        "# In[25]:\n",
        "\n",
        "\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"Loss plot.jpg\")\n",
        "\n",
        "\n",
        "# In[18]:\n",
        "\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    tw = tokenizer.texts_to_sequences([text])\n",
        "    tw = pad_sequences(tw,maxlen=200)\n",
        "    prediction = int(model.predict(tw).round().item())\n",
        "    print(\"Predicted label: \", sentiment_label[1][prediction])\n",
        "\n",
        "\n",
        "# In[19]:\n",
        "\n",
        "\n",
        "test_sentence1 = \"I enjoyed my journey on this flight.\"\n",
        "predict_sentiment(test_sentence1)\n",
        "\n",
        "test_sentence2 = \"This is the worst flight experience of my life!\"\n",
        "predict_sentiment(test_sentence2)\n",
        "\n",
        "\n",
        "# In[ ]:"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Steps to build Sentiment Analysis Text Classifier in Python\n",
        "1. Data Preprocessing\n",
        "As we are dealing with the text data, we need to preprocess it using word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./DesktopDataFlair/Sentiment-Analysis/Tweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tweet_id airline_sentiment airline_sentiment_confidence negativereason negativereason_confidence. airline airline_sentiment_gold\n",
        "name\n",
        "0 570306133677760513\n",
        "neutral\n",
        "1.0000\n",
        "NaN\n",
        "NaN Virgin America\n",
        "NaN\n",
        "cairdin\n",
        "1 570301130888122368\n",
        "positive\n",
        "0.3486\n",
        "NaN\n",
        "NaN jnardino\n",
        "0.0000\n",
        "Virgin America\n",
        "2 570301083672813571\n",
        "neutral\n",
        "0.6837\n",
        "NaN\n",
        "NaN Virgin America\n",
        "NaN yvonnalynn\n",
        "3 570301031407624196\n",
        "negative\n",
        "1.0000\n",
        "Bad Flight\n",
        "0.7033\n",
        "Virgin America\n",
        "NaN jnardino\n",
        "4 570300817074462722\n",
        "negative\n",
        "1.0000\n",
        "Can't Tell\n",
        "1.0000\n",
        "Virgin America\n",
        "NaN\n",
        "jnardino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "We only need the text and sentiment column.\n",
        "\n",
        "review_df = df[['text','airline_sentiment']]\n",
        "\n",
        "print(review_df.shape)\n",
        "review_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "(14640, 2)\n",
        "Out[4]:\n",
        "text airline_sentiment\n",
        "@VirginAmerica What @dhepburn said\n",
        "neutral\n",
        "1 @VirginAmerica plus you've added commercials t...\n",
        "2\n",
        "3\n",
        "4\n",
        "positive\n",
        "@VirginAmerica I didn't today... Must mean I n...\n",
        "neutral\n",
        "@VirginAmerica it's really aggressive to blast....\n",
        "negative\n",
        "@VirginAmerica and it's a really big bad thing.\n",
        "negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "There are more than 14,000 data samples in the sentiment analysis dataset.\n",
        "\n",
        "Let’s check the column names.\n",
        "\n",
        "df.columns\n",
        "\n",
        "Out[23]: Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'airline', 'airline_sentiment_gold', 'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone'], dtype='object')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "We don’t really need neutral reviews in our dataset for this binary classification problem. So, drop those rows from the dataset.\n",
        "\n",
        "review_df = review_df[review_df['airline_sentiment'] != 'neutral']\n",
        "\n",
        "print(review_df.shape)\n",
        "review_df.head(5)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "(11541, 2)\n",
        "Out[22]:\n",
        "text airline_sentiment\n",
        "1 @VirginAmerica plus you've added commercials t...\n",
        "3\n",
        "4\n",
        "5\n",
        "positive\n",
        "@VirginAmerica it's really aggressive to blast...\n",
        "negative\n",
        "@VirginAmerica and it's a really big bad thing...\n",
        "negative\n",
        "@VirginAmerica seriously would pay $30 a fligh...\n",
        "negative\n",
        "6 @VirginAmerica yes, nearly every time I fly VX...\n",
        "positive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "Check the values of the airline_sentiment column.\n",
        "\n",
        "review_df[\"airline_sentiment\"].value_counts()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "The labels for this dataset are categorical. Machines understand only numeric data. So, convert the categorical values to numeric using the factorize() method. This returns an array of numeric values and an Index of categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sentiment_label = review_df.airline_sentiment.factorize()\n",
        "sentiment_label"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "If you observe, the 0 here represents positive sentiment and the 1 represents negative sentiment.\n",
        "\n",
        "Now, the major part in python sentiment analysis. We should transform our text data into something that our machine learning model understands. Basically, we need to convert the text into an array of vector embeddings. Word embeddings are a beautiful way of representing the relationship between the words in the text.\n",
        "\n",
        "To do this, we first give each of the unique words a unique number and then replace that word with the number assigned.\n",
        "\n",
        "First, retrieve all the text data from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tweet = review_df.text.values"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Now, before proceeding ahead in python sentiment analysis project let’s tokenize all the words in the text with the help of Tokenizer. In tokenization, we break down all the words/sentences of a text into small parts called tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "\n",
        "tokenizer.fit_on_texts(tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fit_on_texts() method creates an association between the words and the assigned numbers. This association is stored in the form of a dictionary in the tokenizer.word_index attribute.\n",
        "\n",
        "Now, replace the words with their assigned numbers using the text_to_sequence() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "encoded_docs = tokenizer.texts_to_sequences(tweet)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Each of the sentences in the dataset is not of equal length. Use padding to pad the sentences to have equal length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_sequence = pad_sequences(encoded_docs, maxlen=200)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 2. Build the Text Classifier\n",
        "For sentiment analysis project, we use LSTM layers in the machine learning model. The architecture of our model consists of an embedding layer, an LSTM layer, and a Dense layer at the end. To avoid overfitting, we introduced the Dropout mechanism in-between the LSTM layers.\n",
        "\n",
        "LSTM stands for Long Short Term Memory Networks. It is a variant of Recurrent Neural Networks. Recurrent Neural Networks are usually used with sequential data such as text and audio. Usually, while computing an embedding matrix, the meaning of every word and its calculations (which are called hidden states) are stored. If the reference of a word, let’s say a word is used after 100 words in a text, then all these calculations RNNs cannot store in its memory. That’s why RNNs are not capable of learning these long-term dependencies.\n",
        "\n",
        "LSTMs on the other hand work well with such text. LSTM networks work well with time-series data."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Dropout is one of the regularization techniques. It is used to avoid overfitting. In the dropout mechanism, we drop some neurons randomly. The layer takes an argument, a number between 0 and 1 that represents the probability to drop the neurons. This creates a robust model avoiding overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_vector_length, input_length=200))\n",
        "model.add(SpatialDropout1D(0.25))\n",
        "model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 3. Train the sentiment analysis model\n",
        "Train the sentiment analysis model for 5 epochs on the whole dataset with a batch size of 32 and a validation split of 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=5, batch_size=32)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "The python sentiment analysis model obtained 96% accuracy on the training set and 94.33% accuracy on the test set."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Let’s plot these metrics using the matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.savefig(\"Accuracy plot.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.savefig(\"Loss plt.jpg\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "The above two programe is showing the output in graph"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Let’s execute sentiment analysis model\n",
        "Define a function that takes a text as input and outputs its prediction label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text):\n",
        "    tw = tokenizer.texts_to_sequences([text])\n",
        "    tw = pad_sequences(tw,maxlen=200)\n",
        "    prediction = int(model.predict(tw).round().item())\n",
        "    print(\"Predicted label: \", sentiment_label[1][prediction])\n",
        "\n",
        "\n",
        "test_sentence1 = \"I enjoyed my journey on this flight.\"\n",
        "predict_sentiment(test_sentence1)\n",
        "\n",
        "test_sentence2 = \"This is the worst flight experience of my life!\"\n",
        "predict_sentiment(test_sentence2)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Python Sentiment Analysis Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "In [19]: test_sentence1 = \"I enjoyed my journey on this flight.\" predict_sentiment(test_sentence1)\n",
        "test_sentence2 = \"This is the worst flight experience of my life!\" predict_sentiment(test_sentence2)\n",
        "Predicted label: positive\n",
        "Predicted label:\n",
        "negative"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Summary\n",
        "We have successfully developed python sentiment analysis model. In this machine learning project, we built a binary text classifier that classifies the sentiment of the tweets into positive and negative. We obtained more than 94% accuracy on validation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SQLite",
      "language": "sql",
      "name": "SQLite"
    },
    "language_info": {
      "codemirror_mode": "sql",
      "file_extension": "",
      "mimetype": "",
      "name": "sql",
      "version": "3.32.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
